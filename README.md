# Infraestrutura AWS com Terraform - Modular

Este projeto cont√©m uma infraestrutura AWS modularizada usando Terraform, onde cada componente est√° organizado em sua pr√≥pria pasta para facilitar a manuten√ß√£o e reutiliza√ß√£o.

## Estrutura do Projeto

```
infrastructure-iac/
‚îú‚îÄ‚îÄ main.tf                     # Arquivo principal que chama todos os m√≥dulos
‚îú‚îÄ‚îÄ variables.tf                # Vari√°veis principais do projeto
‚îú‚îÄ‚îÄ outputs.tf                  # Outputs principais da infraestrutura
‚îú‚îÄ‚îÄ modules/                    # Pasta contendo todos os m√≥dulos
‚îÇ   ‚îú‚îÄ‚îÄ vpc/                    # M√≥dulo VPC
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ main.tf            # Variables, Resources e Outputs da VPC
‚îÇ   ‚îú‚îÄ‚îÄ subnets/                # M√≥dulo Subnets
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ main.tf            # Variables, Resources e Outputs das Subnets
‚îÇ   ‚îú‚îÄ‚îÄ internet-gateway/       # M√≥dulo Internet Gateway
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ main.tf            # Variables, Resources e Outputs do IGW
‚îÇ   ‚îú‚îÄ‚îÄ route-tables/           # M√≥dulo Route Tables
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ main.tf            # Variables, Resources e Outputs das Route Tables
‚îÇ   ‚îú‚îÄ‚îÄ security-groups/        # M√≥dulo Security Groups
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ main.tf            # Variables, Resources e Outputs dos Security Groups
‚îÇ   ‚îú‚îÄ‚îÄ ec2/                    # M√≥dulo EC2
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ main.tf            # Variables, Resources e Outputs das inst√¢ncias EC2
‚îÇ   ‚îú‚îÄ‚îÄ s3/                     # M√≥dulo S3
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ main.tf            # Variables, Resources e Outputs dos buckets S3
‚îÇ   ‚îî‚îÄ‚îÄ load-balancer/          # M√≥dulo Load Balancer
‚îÇ       ‚îî‚îÄ‚îÄ main.tf            # Variables, Resources e Outputs do ELB
```

## Recursos Criados

### VPC e Rede
- **VPC**: Rede virtual privada (10.0.0.0/24)
- **Subnets**: 
  - Subnet p√∫blica (10.0.0.0/25)
  - Subnet privada (10.0.0.128/25)
- **Internet Gateway**: Para acesso √† internet
- **Route Tables**: Tabelas de roteamento para subnets

### Seguran√ßa
- **Security Groups**:
  - SG P√∫blico: Permite SSH (porta 22) de qualquer IP
  - SG Privado: Permite SSH apenas da VPC

### Computa√ß√£o
- **EC2 Instances**:
  - Inst√¢ncia p√∫blica (com IP p√∫blico)
  - Inst√¢ncia privada (apenas IP privado)
- **Load Balancer**: ELB cl√°ssico na subnet p√∫blica

### Armazenamento
- **S3 Buckets**:
  - **aej-public-bucket**: Bucket p√∫blico para website e processamento Excel
  - **aej-staging-bucket**: Dados brutos do pipeline ETL
  - **aej-trusted-bucket**: Dados limpos (colunas filtradas)
  - **aej-cured-bucket**: Dados refinados (apenas livros)

### Processamento de Dados
- **Lambda Functions**:
  - **excel-processor**: Processa arquivos Excel enviados ao S3
  - **staging-to-trusted-etl**: Filtra colunas espec√≠ficas dos dados brutos
  - **trusted-to-cured-etl**: Filtra apenas registros de livros

## Como Usar

### Pr√©-requisitos
- Terraform instalado (>= 1.2)
- AWS CLI configurado com credenciais v√°lidas
- Acesso √† AWS com permiss√µes adequadas

### Comandos B√°sicos

1. **Inicializar o Terraform**:
   ```bash
   terraform init
   ```

2. **Aplicar a infraestrutura**:
   ```bash
   terraform apply
   ```

3. **Destruir a infraestrutura**:
   ```bash
   terraform destroy
   ```

### Customiza√ß√£o

Voc√™ pode customizar a infraestrutura editando as vari√°veis em `variables.tf` ou passando valores via linha de comando:

```bash
terraform apply -var="aws_region=us-west-2" -var="environment=prod"
```

## Vantagens da Estrutura Simplificada

1. **Simplicidade**: Cada m√≥dulo tem apenas um arquivo, facilitando a navega√ß√£o
2. **Reutiliza√ß√£o**: M√≥dulos podem ser reutilizados em diferentes projetos
3. **Manutenibilidade**: Cada componente est√° isolado e pode ser atualizado independentemente
4. **Legibilidade**: C√≥digo mais organizado e f√°cil de entender
5. **Funcionalidade Completa**: Cada arquivo `main.tf` cont√©m tudo necess√°rio (variables, resources, outputs)

## M√≥dulos Funcionais

Cada m√≥dulo √© completamente funcional e autocontido em um √∫nico arquivo `main.tf`, que inclui:
- **Variables**: Defini√ß√µes de vari√°veis de entrada
- **Resources**: Recursos da AWS a serem criados
- **Outputs**: Valores de sa√≠da do m√≥dulo

### Exemplo de uso individual de um m√≥dulo:

```hcl
module "vpc_example" {
  source = "./modules/vpc"
  
  vpc_cidr = "10.1.0.0/16"
  vpc_name = "my-custom-vpc"
}
```

## Outputs Dispon√≠veis

A infraestrutura exp√µe v√°rios outputs √∫teis que podem ser consumidos por outros projetos ou usados para refer√™ncia:

- IDs de recursos (VPC, subnets, inst√¢ncias, etc.)
- IPs das inst√¢ncias
- DNS do Load Balancer
- IDs dos buckets S3

## Sistema de Processamento de Excel via Lambda e S3

Este projeto inclui uma fun√ß√£o Lambda que processa automaticamente arquivos Excel (.xlsx) enviados para o bucket S3.

### üìã Funcionalidades

- **Trigger Autom√°tico**: O Lambda √© acionado automaticamente quando arquivos `.xlsx` s√£o enviados para a pasta `datasets/` no S3
- **Processamento Ass√≠ncrono**: Arquivos s√£o processados em background sem necessidade de interven√ß√£o
- **Relat√≥rios JSON**: Gera relat√≥rios de processamento na pasta `outputs/` do bucket
- **Logs Detalhados**: CloudWatch mant√©m logs de todas as execu√ß√µes

### üöÄ Como Usar

#### 1. Ap√≥s o Deploy da Infraestrutura

Execute o Terraform para criar todos os recursos:

```bash
cd Codigos-IaC
terraform init
terraform apply -auto-approve
```

#### 2. Obter Informa√ß√µes do Bucket

Ap√≥s o deploy, veja as informa√ß√µes importantes:

```bash
terraform output
```

Voc√™ ver√° informa√ß√µes como:
- `s3_bucket_name`: Nome do bucket S3 criado
- `excel_lambda_function_name`: Nome da fun√ß√£o Lambda
- `excel_processing_instructions`: Instru√ß√µes de uso
- `s3_website_endpoint`: Endpoint p√∫blico do bucket

#### 3. Enviar Arquivos Excel para Processamento

**Usando AWS CLI:**

```bash
# Enviar um √∫nico arquivo
aws s3 cp seu_arquivo.xlsx s3://aej-public-bucket-XXXXXX/datasets/

# Enviar m√∫ltiplos arquivos
aws s3 cp arquivo1.xlsx s3://aej-public-bucket-XXXXXX/datasets/
aws s3 cp arquivo2.xlsx s3://aej-public-bucket-XXXXXX/datasets/

# Enviar uma pasta inteira
aws s3 cp ./meus_excels/ s3://aej-public-bucket-XXXXXX/datasets/ --recursive
```

**Usando Console AWS:**
1. Acesse o S3 Console
2. Navegue at√© o bucket `aej-public-bucket-XXXXXX`
3. Entre na pasta `datasets/`
4. Clique em "Upload" e selecione seus arquivos `.xlsx`

#### 4. Verificar os Resultados

**Listar arquivos processados:**

```bash
# Ver relat√≥rios gerados
aws s3 ls s3://aej-public-bucket-XXXXXX/outputs/

# Baixar um relat√≥rio espec√≠fico
aws s3 cp s3://aej-public-bucket-XXXXXX/outputs/processing_report_2025-10-19_21-33-09.json .

# Baixar todos os resultados
aws s3 cp s3://aej-public-bucket-XXXXXX/outputs/ ./resultados/ --recursive
```

**Ver logs da execu√ß√£o:**

```bash
# Listar √∫ltimas execu√ß√µes do Lambda
aws logs describe-log-streams \
  --log-group-name "/aws/lambda/excel-processor-terraform" \
  --order-by LastEventTime \
  --descending \
  --max-items 5

# Ver logs de uma execu√ß√£o espec√≠fica
aws logs filter-log-events \
  --log-group-name "/aws/lambda/excel-processor-terraform" \
  --start-time <timestamp_em_ms>
```

#### 5. Estrutura de Pastas no S3

```
s3://aej-public-bucket-XXXXXX/
‚îú‚îÄ‚îÄ datasets/              # ‚Üê Coloque seus arquivos .xlsx aqui
‚îÇ   ‚îú‚îÄ‚îÄ arquivo1.xlsx
‚îÇ   ‚îú‚îÄ‚îÄ arquivo2.xlsx
‚îÇ   ‚îî‚îÄ‚îÄ tabelao_tratado.xlsx
‚îî‚îÄ‚îÄ outputs/               # ‚Üê Relat√≥rios processados aparecem aqui
    ‚îú‚îÄ‚îÄ processing_report_2025-10-19_21-29-53.json
    ‚îî‚îÄ‚îÄ processing_report_2025-10-19_21-33-09.json
```

### üîß Arquivos do Lambda

- **`excel_processor_lambda.py`**: C√≥digo Python da fun√ß√£o Lambda
- **`excel_processor_lambda.zip`**: Pacote ZIP criado automaticamente pelo Terraform

### ‚öôÔ∏è Configura√ß√µes Importantes

**Recursos do Lambda:**
- Runtime: Python 3.9
- Mem√≥ria: 3008 MB (m√°ximo dispon√≠vel)
- Timeout: 900 segundos (15 minutos)
- Trigger: S3 ObjectCreated em `datasets/*.xlsx`

**Permiss√µes:**
- Usa `LabRole` existente no AWS Labs
- Acesso de leitura/escrita no bucket S3
- Logs no CloudWatch

### üßπ Limpeza e Destroy

**Importante**: Antes de destruir a infraestrutura, esvazie o bucket S3:

```bash
# Remover todos os arquivos do bucket
aws s3 rm s3://aej-public-bucket-XXXXXX --recursive

# Depois destruir a infraestrutura
terraform destroy -auto-approve
```

**Ou configure force_destroy no Terraform** (opcional):

No arquivo `infra__aej.tf`, adicione `force_destroy = true` no recurso do bucket:

```hcl
resource "aws_s3_bucket" "aej_public" {
  bucket        = "aej-public-bucket-${random_id.bucket_suffix.hex}"
  force_destroy = true  # ‚Üê Remove automaticamente objetos no destroy
}
```

‚ö†Ô∏è **Aten√ß√£o**: Com `force_destroy = true`, todos os arquivos ser√£o deletados automaticamente ao executar `terraform destroy`.

### üìä Exemplo de Relat√≥rio Gerado

```json
{
  "timestamp": "2025-10-19_21-33-09",
  "trigger_file": "datasets/tabelao_tratado.xlsx",
  "total_excel_files": 2,
  "excel_files_list": [
    "datasets/teste.xlsx",
    "datasets/tabelao_tratado.xlsx"
  ],
  "status": "detected",
  "message": "Excel files detected - ready for processing"
}
```

### üêõ Troubleshooting

**Problema**: Lambda n√£o est√° sendo acionado
- Verifique se o arquivo tem extens√£o `.xlsx`
- Confirme que est√° enviando para a pasta `datasets/`
- Verifique os logs no CloudWatch

**Problema**: Erro de permiss√£o ao fazer destroy
- Execute: `aws s3 rm s3://seu-bucket --recursive`
- Depois execute: `terraform destroy`

**Problema**: Arquivo muito grande (timeout)
- O timeout est√° configurado para 15 minutos (m√°ximo)
- Para arquivos gigantes, considere aumentar recursos ou dividir o arquivo

---

## Pipeline ETL Automatizado (Staging ‚Üí Trusted ‚Üí Cured)

Este projeto inclui um pipeline ETL completo e automatizado com tr√™s est√°gios de processamento de dados.

### üìä Arquitetura do Pipeline

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      Lambda 1       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      Lambda 2       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   STAGING   ‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ> ‚îÇ   TRUSTED   ‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ> ‚îÇ    CURED    ‚îÇ
‚îÇ (dados raw) ‚îÇ   Filtra Colunas    ‚îÇ (dados      ‚îÇ   Filtra Livros     ‚îÇ (livros     ‚îÇ
‚îÇ             ‚îÇ   Preenche null     ‚îÇ  limpos)    ‚îÇ   Preenche null     ‚îÇ  apenas)    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
     CSV/XLSX                            CSV                                  CSV
```

### üîÑ Est√°gios do Pipeline

#### **Est√°gio 1: STAGING (Dados Brutos)**
Armazena dados originais sem processamento.

#### **Est√°gio 2: TRUSTED (Dados Limpos)**
**Lambda**: `staging-to-trusted-etl`

**Transforma√ß√µes aplicadas:**
- Filtra **apenas** as seguintes colunas:
  - `data`
  - `dia da semana`
  - `feriado`
  - `product_category_name`
  - `seller_city`
  - `seller_state`
  - `quantidade`
  - `obra vendida`
  - `valor pago`
  - `forma de pagamento`
- Preenche campos vazios com `null`
- Normaliza nomes de colunas (case-insensitive)

#### **Est√°gio 3: CURED (Dados Refinados)**
**Lambda**: `trusted-to-cured-etl`

**Transforma√ß√µes aplicadas:**
- Mant√©m **apenas** registros onde `product_category_name` cont√©m:
  - `livro`
  - `book`
  - `literatura`
- Remove todas as outras categorias
- Mant√©m preenchimento de `null` para campos vazios
- Gera estat√≠sticas de processamento

### üöÄ Como Usar o Pipeline ETL

#### 1. **Enviar Dados Brutos ao Staging**

```bash
# Enviar arquivo CSV
aws s3 cp meus_dados.csv s3://aej-staging-bucket-XXXXXX/

# Enviar m√∫ltiplos arquivos
aws s3 cp ./dados/ s3://aej-staging-bucket-XXXXXX/ --recursive --exclude "*" --include "*.csv"
```

‚ö†Ô∏è **Importante**: Use arquivos **CSV** para melhor compatibilidade. Excel (.xlsx) requer layer com pandas.

#### 2. **Aguardar Processamento Autom√°tico**

O pipeline √© totalmente autom√°tico:

1. **Lambda 1** detecta arquivo no Staging ‚Üí processa ‚Üí salva no Trusted
2. **Lambda 2** detecta arquivo no Trusted ‚Üí processa ‚Üí salva no Cured

```bash
# Monitorar logs em tempo real
aws logs tail /aws/lambda/staging-to-trusted-etl --follow
aws logs tail /aws/lambda/trusted-to-cured-etl --follow
```

#### 3. **Baixar Dados Processados**

```bash
# Listar arquivos processados
aws s3 ls s3://aej-trusted-bucket-XXXXXX/trusted/
aws s3 ls s3://aej-cured-bucket-XXXXXX/cured/

# Baixar dados limpos (todas colunas filtradas)
aws s3 cp s3://aej-trusted-bucket-XXXXXX/trusted/ ./dados_trusted/ --recursive

# Baixar dados refinados (apenas livros)
aws s3 cp s3://aej-cured-bucket-XXXXXX/cured/ ./dados_cured/ --recursive
```

### üìÅ Estrutura de Arquivos nos Buckets

```
üì¶ aej-staging-bucket-XXXXXX/
‚îî‚îÄ‚îÄ vendas_2024.csv                    # Dados originais

üì¶ aej-trusted-bucket-XXXXXX/
‚îî‚îÄ‚îÄ trusted/
    ‚îî‚îÄ‚îÄ vendas_2024_trusted_20251019_143022.csv   # Colunas filtradas

üì¶ aej-cured-bucket-XXXXXX/
‚îî‚îÄ‚îÄ cured/
    ‚îî‚îÄ‚îÄ vendas_2024_cured_20251019_143025.csv     # Apenas livros
```

### üìã Exemplo de Transforma√ß√£o

**Entrada (Staging):**
```csv
data,dia da semana,feriado,product_category_name,seller_city,quantidade,outra_coluna
2024-01-01,Segunda,Sim,livros_tecnicos,S√£o Paulo,5,valor_ignorado
2024-01-02,Ter√ßa,N√£o,eletronicos,Rio de Janeiro,3,outro_valor
2024-01-03,Quarta,N√£o,livros_ficcao,Curitiba,2,mais_dados
```

**Sa√≠da Trusted (colunas filtradas):**
```csv
data,dia da semana,feriado,product_category_name,seller_city,seller_state,quantidade,obra vendida,valor pago,forma de pagamento
2024-01-01,Segunda,Sim,livros_tecnicos,S√£o Paulo,null,5,null,null,null
2024-01-02,Ter√ßa,N√£o,eletronicos,Rio de Janeiro,null,3,null,null,null
2024-01-03,Quarta,N√£o,livros_ficcao,Curitiba,null,2,null,null,null
```

**Sa√≠da Cured (apenas livros):**
```csv
data,dia da semana,feriado,product_category_name,seller_city,seller_state,quantidade,obra vendida,valor pago,forma de pagamento
2024-01-01,Segunda,Sim,livros_tecnicos,S√£o Paulo,null,5,null,null,null
2024-01-03,Quarta,N√£o,livros_ficcao,Curitiba,null,2,null,null,null
```

### ‚öôÔ∏è Configura√ß√µes dos Lambdas ETL

| Lambda | Mem√≥ria | Timeout | Trigger | Output |
|--------|---------|---------|---------|--------|
| staging-to-trusted | 512 MB | 5 min | S3 `.csv` no Staging | Trusted bucket |
| trusted-to-cured | 512 MB | 5 min | S3 `.csv` no Trusted | Cured bucket |

### üîç Monitoramento e Logs

**Ver execu√ß√µes recentes:**
```bash
# Listar streams de log
aws logs describe-log-streams \
  --log-group-name "/aws/lambda/staging-to-trusted-etl" \
  --order-by LastEventTime \
  --descending \
  --max-items 5

# Filtrar logs por per√≠odo
aws logs filter-log-events \
  --log-group-name "/aws/lambda/trusted-to-cured-etl" \
  --start-time $(date -d '1 hour ago' +%s)000
```

**Logs incluem:**
- Arquivo de origem processado
- Colunas encontradas e mapeadas
- N√∫mero de linhas processadas
- Estat√≠sticas (total, aceitas, descartadas)
- Localiza√ß√£o do arquivo de sa√≠da

### üßπ Limpeza dos Buckets ETL

Antes de destruir a infraestrutura:

```bash
# Esvaziar todos os buckets ETL
aws s3 rm s3://aej-staging-bucket-XXXXXX --recursive
aws s3 rm s3://aej-trusted-bucket-XXXXXX --recursive
aws s3 rm s3://aej-cured-bucket-XXXXXX --recursive

# Depois destruir
terraform destroy -auto-approve
```

**Ou configure `force_destroy = true`** (j√° configurado por padr√£o nos buckets ETL).

### üêõ Troubleshooting ETL

**Problema**: Lambda n√£o est√° processando
- Verifique se o arquivo √© `.csv` (Excel requer layer pandas)
- Confirme que enviou para o bucket correto (staging)
- Verifique logs no CloudWatch

**Problema**: Colunas n√£o encontradas
- O Lambda normaliza nomes (case-insensitive)
- Verifique se as colunas existem no CSV original
- Veja logs para mapeamento de colunas

**Problema**: Nenhum livro no Cured
- Verifique se `product_category_name` cont√©m 'livro', 'book' ou 'literatura'
- Veja estat√≠sticas nos logs do Lambda

**Problema**: Arquivo muito grande
- Aumente `timeout` e `memory_size` dos Lambdas no Terraform
- Considere dividir arquivos grandes em chunks menores

### üí° Dicas de Uso

1. **Use CSV em vez de Excel** para melhor performance e compatibilidade
2. **Monitore os logs** durante o primeiro processamento para validar mapeamento de colunas
3. **Teste com arquivo pequeno** primeiro para validar o pipeline
4. **Revise os outputs** do Terraform para ver nomes dos buckets criados

---
