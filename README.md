# Infraestrutura AWS com Terraform - Modular

Este projeto contÃ©m uma infraestrutura AWS modularizada usando Terraform, onde cada componente estÃ¡ organizado em sua prÃ³pria pasta para facilitar a manutenÃ§Ã£o e reutilizaÃ§Ã£o.

## Estrutura do Projeto

```
infrastructure-iac/
â”œâ”€â”€ main.tf                     # Arquivo principal que chama todos os mÃ³dulos
â”œâ”€â”€ variables.tf                # VariÃ¡veis principais do projeto
â”œâ”€â”€ outputs.tf                  # Outputs principais da infraestrutura
â”œâ”€â”€ modules/                    # Pasta contendo todos os mÃ³dulos
â”‚   â”œâ”€â”€ vpc/                    # MÃ³dulo VPC
â”‚   â”‚   â””â”€â”€ main.tf            # Variables, Resources e Outputs da VPC
â”‚   â”œâ”€â”€ subnets/                # MÃ³dulo Subnets
â”‚   â”‚   â””â”€â”€ main.tf            # Variables, Resources e Outputs das Subnets
â”‚   â”œâ”€â”€ internet-gateway/       # MÃ³dulo Internet Gateway
â”‚   â”‚   â””â”€â”€ main.tf            # Variables, Resources e Outputs do IGW
â”‚   â”œâ”€â”€ route-tables/           # MÃ³dulo Route Tables
â”‚   â”‚   â””â”€â”€ main.tf            # Variables, Resources e Outputs das Route Tables
â”‚   â”œâ”€â”€ security-groups/        # MÃ³dulo Security Groups
â”‚   â”‚   â””â”€â”€ main.tf            # Variables, Resources e Outputs dos Security Groups
â”‚   â”œâ”€â”€ ec2/                    # MÃ³dulo EC2
â”‚   â”‚   â””â”€â”€ main.tf            # Variables, Resources e Outputs das instÃ¢ncias EC2
â”‚   â”œâ”€â”€ s3/                     # MÃ³dulo S3
â”‚   â”‚   â””â”€â”€ main.tf            # Variables, Resources e Outputs dos buckets S3
â”‚   â””â”€â”€ load-balancer/          # MÃ³dulo Load Balancer
â”‚       â””â”€â”€ main.tf            # Variables, Resources e Outputs do ELB
```

## Recursos Criados

### VPC e Rede
- **VPC**: Rede virtual privada (10.0.0.0/24)
- **Subnets**: 
  - Subnet pÃºblica (10.0.0.0/25)
  - Subnet privada (10.0.0.128/25)
- **Internet Gateway**: Para acesso Ã  internet
- **Route Tables**: Tabelas de roteamento para subnets

### SeguranÃ§a
- **Security Groups**:
  - SG PÃºblico: Permite SSH (porta 22) de qualquer IP
  - SG Privado: Permite SSH apenas da VPC

### ComputaÃ§Ã£o
- **EC2 Instances**:
  - InstÃ¢ncia pÃºblica (com IP pÃºblico)
  - InstÃ¢ncia privada (apenas IP privado)
- **Load Balancer**: ELB clÃ¡ssico na subnet pÃºblica

### Armazenamento
- **S3 Buckets**:
  - **aej-public-bucket**: Bucket pÃºblico para website e processamento Excel
  - **aej-staging-bucket**: Dados brutos do pipeline ETL
  - **aej-trusted-bucket**: Dados limpos (colunas filtradas)
  - **aej-cured-bucket**: Dados refinados (apenas livros)

### Processamento de Dados
- **Lambda Functions**:
  - **excel-processor**: Processa arquivos Excel enviados ao S3
  - **staging-to-trusted-etl**: Filtra colunas especÃ­ficas dos dados brutos
  - **trusted-to-cured-etl**: Filtra apenas registros de livros

## Como Usar

### PrÃ©-requisitos
- Terraform instalado (>= 1.2)
- AWS CLI configurado com credenciais vÃ¡lidas
- Acesso Ã  AWS com permissÃµes adequadas

### Comandos BÃ¡sicos

1. **Inicializar o Terraform**:
   ```bash
   terraform init
   ```

2. **Aplicar a infraestrutura**:
   ```bash
   terraform apply
   ```

3. **Destruir a infraestrutura**:
   ```bash
   terraform destroy
   ```

### CustomizaÃ§Ã£o

VocÃª pode customizar a infraestrutura editando as variÃ¡veis em `variables.tf` ou passando valores via linha de comando:

```bash
terraform apply -var="aws_region=us-west-2" -var="environment=prod"
```

## Vantagens da Estrutura Simplificada

1. **Simplicidade**: Cada mÃ³dulo tem apenas um arquivo, facilitando a navegaÃ§Ã£o
2. **ReutilizaÃ§Ã£o**: MÃ³dulos podem ser reutilizados em diferentes projetos
3. **Manutenibilidade**: Cada componente estÃ¡ isolado e pode ser atualizado independentemente
4. **Legibilidade**: CÃ³digo mais organizado e fÃ¡cil de entender
5. **Funcionalidade Completa**: Cada arquivo `main.tf` contÃ©m tudo necessÃ¡rio (variables, resources, outputs)

## MÃ³dulos Funcionais

Cada mÃ³dulo Ã© completamente funcional e autocontido em um Ãºnico arquivo `main.tf`, que inclui:
- **Variables**: DefiniÃ§Ãµes de variÃ¡veis de entrada
- **Resources**: Recursos da AWS a serem criados
- **Outputs**: Valores de saÃ­da do mÃ³dulo

### Exemplo de uso individual de um mÃ³dulo:

```hcl
module "vpc_example" {
  source = "./modules/vpc"
  
  vpc_cidr = "10.1.0.0/16"
  vpc_name = "my-custom-vpc"
}
```

## Outputs DisponÃ­veis

A infraestrutura expÃµe vÃ¡rios outputs Ãºteis que podem ser consumidos por outros projetos ou usados para referÃªncia:

- IDs de recursos (VPC, subnets, instÃ¢ncias, etc.)
- IPs das instÃ¢ncias
- DNS do Load Balancer
- IDs dos buckets S3

## Sistema de Processamento de Excel via Lambda e S3

Este projeto inclui uma funÃ§Ã£o Lambda que processa automaticamente arquivos Excel (.xlsx) enviados para o bucket S3.

### ğŸ“‹ Funcionalidades

- **Trigger AutomÃ¡tico**: O Lambda Ã© acionado automaticamente quando arquivos `.xlsx` sÃ£o enviados para a pasta `datasets/` no S3
- **Processamento AssÃ­ncrono**: Arquivos sÃ£o processados em background sem necessidade de intervenÃ§Ã£o
- **RelatÃ³rios JSON**: Gera relatÃ³rios de processamento na pasta `outputs/` do bucket
- **Logs Detalhados**: CloudWatch mantÃ©m logs de todas as execuÃ§Ãµes

### ğŸš€ Como Usar

#### 1. ApÃ³s o Deploy da Infraestrutura

Execute o Terraform para criar todos os recursos:

```bash
cd Codigos-IaC
terraform init
terraform apply -auto-approve
```

#### 2. Obter InformaÃ§Ãµes do Bucket

ApÃ³s o deploy, veja as informaÃ§Ãµes importantes:

```bash
terraform output
```

VocÃª verÃ¡ informaÃ§Ãµes como:
- `s3_bucket_name`: Nome do bucket S3 criado
- `excel_lambda_function_name`: Nome da funÃ§Ã£o Lambda
- `excel_processing_instructions`: InstruÃ§Ãµes de uso
- `s3_website_endpoint`: Endpoint pÃºblico do bucket

#### 3. Enviar Arquivos Excel para Processamento

**Usando AWS CLI:**

```bash
# Enviar um Ãºnico arquivo
aws s3 cp seu_arquivo.xlsx s3://aej-public-bucket-XXXXXX/datasets/

# Enviar mÃºltiplos arquivos
aws s3 cp arquivo1.xlsx s3://aej-public-bucket-XXXXXX/datasets/
aws s3 cp arquivo2.xlsx s3://aej-public-bucket-XXXXXX/datasets/

# Enviar uma pasta inteira
aws s3 cp ./meus_excels/ s3://aej-public-bucket-XXXXXX/datasets/ --recursive
```

**Usando Console AWS:**
1. Acesse o S3 Console
2. Navegue atÃ© o bucket `aej-public-bucket-XXXXXX`
3. Entre na pasta `datasets/`
4. Clique em "Upload" e selecione seus arquivos `.xlsx`

#### 4. Verificar os Resultados

**Listar arquivos processados:**

```bash
# Ver relatÃ³rios gerados
aws s3 ls s3://aej-public-bucket-XXXXXX/outputs/

# Baixar um relatÃ³rio especÃ­fico
aws s3 cp s3://aej-public-bucket-XXXXXX/outputs/processing_report_2025-10-19_21-33-09.json .

# Baixar todos os resultados
aws s3 cp s3://aej-public-bucket-XXXXXX/outputs/ ./resultados/ --recursive
```

**Ver logs da execuÃ§Ã£o:**

```bash
# Listar Ãºltimas execuÃ§Ãµes do Lambda
aws logs describe-log-streams \
  --log-group-name "/aws/lambda/excel-processor-terraform" \
  --order-by LastEventTime \
  --descending \
  --max-items 5

# Ver logs de uma execuÃ§Ã£o especÃ­fica
aws logs filter-log-events \
  --log-group-name "/aws/lambda/excel-processor-terraform" \
  --start-time <timestamp_em_ms>
```

#### 5. Estrutura de Pastas no S3

```
s3://aej-public-bucket-XXXXXX/
â”œâ”€â”€ datasets/              # â† Coloque seus arquivos .xlsx aqui
â”‚   â”œâ”€â”€ arquivo1.xlsx
â”‚   â”œâ”€â”€ arquivo2.xlsx
â”‚   â””â”€â”€ tabelao_tratado.xlsx
â””â”€â”€ outputs/               # â† RelatÃ³rios processados aparecem aqui
    â”œâ”€â”€ processing_report_2025-10-19_21-29-53.json
    â””â”€â”€ processing_report_2025-10-19_21-33-09.json
```

### ğŸ”§ Arquivos do Lambda

- **`excel_processor_lambda.py`**: CÃ³digo Python da funÃ§Ã£o Lambda
- **`excel_processor_lambda.zip`**: Pacote ZIP criado automaticamente pelo Terraform

### âš™ï¸ ConfiguraÃ§Ãµes Importantes

**Recursos do Lambda:**
- Runtime: Python 3.9
- MemÃ³ria: 3008 MB (mÃ¡ximo disponÃ­vel)
- Timeout: 900 segundos (15 minutos)
- Trigger: S3 ObjectCreated em `datasets/*.xlsx`

**PermissÃµes:**
- Usa `LabRole` existente no AWS Labs
- Acesso de leitura/escrita no bucket S3
- Logs no CloudWatch

### ğŸ§¹ Limpeza e Destroy

**Importante**: Antes de destruir a infraestrutura, esvazie o bucket S3:

```bash
# Remover todos os arquivos do bucket
aws s3 rm s3://aej-public-bucket-XXXXXX --recursive

# Depois destruir a infraestrutura
terraform destroy -auto-approve
```

**Ou configure force_destroy no Terraform** (opcional):

No arquivo `infra__aej.tf`, adicione `force_destroy = true` no recurso do bucket:

```hcl
resource "aws_s3_bucket" "aej_public" {
  bucket        = "aej-public-bucket-${random_id.bucket_suffix.hex}"
  force_destroy = true  # â† Remove automaticamente objetos no destroy
}
```

âš ï¸ **AtenÃ§Ã£o**: Com `force_destroy = true`, todos os arquivos serÃ£o deletados automaticamente ao executar `terraform destroy`.

### ğŸ“Š Exemplo de RelatÃ³rio Gerado

```json
{
  "timestamp": "2025-10-19_21-33-09",
  "trigger_file": "datasets/tabelao_tratado.xlsx",
  "total_excel_files": 2,
  "excel_files_list": [
    "datasets/teste.xlsx",
    "datasets/tabelao_tratado.xlsx"
  ],
  "status": "detected",
  "message": "Excel files detected - ready for processing"
}
```

### ğŸ› Troubleshooting

**Problema**: Lambda nÃ£o estÃ¡ sendo acionado
- Verifique se o arquivo tem extensÃ£o `.xlsx`
- Confirme que estÃ¡ enviando para a pasta `datasets/`
- Verifique os logs no CloudWatch

**Problema**: Erro de permissÃ£o ao fazer destroy
- Execute: `aws s3 rm s3://seu-bucket --recursive`
- Depois execute: `terraform destroy`

**Problema**: Arquivo muito grande (timeout)
- O timeout estÃ¡ configurado para 15 minutos (mÃ¡ximo)
- Para arquivos gigantes, considere aumentar recursos ou dividir o arquivo

---

## Pipeline ETL Automatizado (Staging â†’ Trusted â†’ Cured)

Este projeto inclui um pipeline ETL completo e automatizado com trÃªs estÃ¡gios de processamento de dados.

### ğŸ“Š Arquitetura do Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      Lambda 1       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      Lambda 2       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   STAGING   â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€> â”‚   TRUSTED   â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€> â”‚    CURED    â”‚
â”‚ (dados raw) â”‚   Filtra Colunas    â”‚ (dados      â”‚   Filtra Livros     â”‚ (livros     â”‚
â”‚             â”‚   Preenche null     â”‚  limpos)    â”‚   Preenche null     â”‚  apenas)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     CSV/XLSX                            CSV                                  CSV
```

### ğŸ”„ EstÃ¡gios do Pipeline

#### **EstÃ¡gio 1: STAGING (Dados Brutos)**
Armazena dados originais sem processamento.

#### **EstÃ¡gio 2: TRUSTED (Dados Limpos)**
**Lambda**: `staging-to-trusted-etl`

**TransformaÃ§Ãµes aplicadas:**
- Filtra **apenas** as seguintes colunas:
  - `data`
  - `dia da semana`
  - `feriado`
  - `product_category_name`
  - `seller_city`
  - `seller_state`
  - `quantidade`
  - `obra vendida`
  - `valor pago`
  - `forma de pagamento`
- Preenche campos vazios com `null`
- Normaliza nomes de colunas (case-insensitive)

#### **EstÃ¡gio 3: CURED (Dados Refinados)**
**Lambda**: `trusted-to-cured-etl`

**TransformaÃ§Ãµes aplicadas:**
- MantÃ©m **apenas** registros onde `product_category_name` contÃ©m:
  - `livro`
  - `book`
  - `literatura`
- Remove todas as outras categorias
- MantÃ©m preenchimento de `null` para campos vazios
- Gera estatÃ­sticas de processamento

### ğŸš€ Como Usar o Pipeline ETL

#### 1. **Enviar Dados Brutos ao Staging**

```bash
# Enviar arquivo CSV
aws s3 cp meus_dados.csv s3://aej-staging-bucket-XXXXXX/

# Enviar mÃºltiplos arquivos
aws s3 cp ./dados/ s3://aej-staging-bucket-XXXXXX/ --recursive --exclude "*" --include "*.csv"
```

âš ï¸ **Importante**: Use arquivos **CSV** para melhor compatibilidade. Excel (.xlsx) requer layer com pandas.

#### 2. **Aguardar Processamento AutomÃ¡tico**

O pipeline Ã© totalmente automÃ¡tico:

1. **Lambda 1** detecta arquivo no Staging â†’ processa â†’ salva no Trusted
2. **Lambda 2** detecta arquivo no Trusted â†’ processa â†’ salva no Cured

```bash
# Monitorar logs em tempo real
aws logs tail /aws/lambda/staging-to-trusted-etl --follow
aws logs tail /aws/lambda/trusted-to-cured-etl --follow
```

#### 3. **Baixar Dados Processados**

```bash
# Listar arquivos processados
aws s3 ls s3://aej-trusted-bucket-XXXXXX/trusted/
aws s3 ls s3://aej-cured-bucket-XXXXXX/cured/

# Baixar dados limpos (todas colunas filtradas)
aws s3 cp s3://aej-trusted-bucket-XXXXXX/trusted/ ./dados_trusted/ --recursive

# Baixar dados refinados (apenas livros)
aws s3 cp s3://aej-cured-bucket-XXXXXX/cured/ ./dados_cured/ --recursive
```

### ğŸ“ Estrutura de Arquivos nos Buckets

```
ğŸ“¦ aej-staging-bucket-XXXXXX/
â””â”€â”€ vendas_2024.csv                    # Dados originais

ğŸ“¦ aej-trusted-bucket-XXXXXX/
â””â”€â”€ trusted/
    â””â”€â”€ vendas_2024_trusted_20251019_143022.csv   # Colunas filtradas

ğŸ“¦ aej-cured-bucket-XXXXXX/
â””â”€â”€ cured/
    â””â”€â”€ vendas_2024_cured_20251019_143025.csv     # Apenas livros
```

### ğŸ“‹ Exemplo de TransformaÃ§Ã£o

**Entrada (Staging):**
```csv
data,dia da semana,feriado,product_category_name,seller_city,quantidade,outra_coluna
2024-01-01,Segunda,Sim,livros_tecnicos,SÃ£o Paulo,5,valor_ignorado
2024-01-02,TerÃ§a,NÃ£o,eletronicos,Rio de Janeiro,3,outro_valor
2024-01-03,Quarta,NÃ£o,livros_ficcao,Curitiba,2,mais_dados
```

**SaÃ­da Trusted (colunas filtradas):**
```csv
data,dia da semana,feriado,product_category_name,seller_city,seller_state,quantidade,obra vendida,valor pago,forma de pagamento
2024-01-01,Segunda,Sim,livros_tecnicos,SÃ£o Paulo,null,5,null,null,null
2024-01-02,TerÃ§a,NÃ£o,eletronicos,Rio de Janeiro,null,3,null,null,null
2024-01-03,Quarta,NÃ£o,livros_ficcao,Curitiba,null,2,null,null,null
```

**SaÃ­da Cured (apenas livros):**
```csv
data,dia da semana,feriado,product_category_name,seller_city,seller_state,quantidade,obra vendida,valor pago,forma de pagamento
2024-01-01,Segunda,Sim,livros_tecnicos,SÃ£o Paulo,null,5,null,null,null
2024-01-03,Quarta,NÃ£o,livros_ficcao,Curitiba,null,2,null,null,null
```

### âš™ï¸ ConfiguraÃ§Ãµes dos Lambdas ETL

| Lambda | MemÃ³ria | Timeout | Trigger | Output |
|--------|---------|---------|---------|--------|
| staging-to-trusted | 512 MB | 5 min | S3 `.csv` no Staging | Trusted bucket |
| trusted-to-cured | 512 MB | 5 min | S3 `.csv` no Trusted | Cured bucket |

### ğŸ” Monitoramento e Logs

**Ver execuÃ§Ãµes recentes:**
```bash
# Listar streams de log
aws logs describe-log-streams \
  --log-group-name "/aws/lambda/staging-to-trusted-etl" \
  --order-by LastEventTime \
  --descending \
  --max-items 5

# Filtrar logs por perÃ­odo
aws logs filter-log-events \
  --log-group-name "/aws/lambda/trusted-to-cured-etl" \
  --start-time $(date -d '1 hour ago' +%s)000
```

**Logs incluem:**
- Arquivo de origem processado
- Colunas encontradas e mapeadas
- NÃºmero de linhas processadas
- EstatÃ­sticas (total, aceitas, descartadas)
- LocalizaÃ§Ã£o do arquivo de saÃ­da

### ğŸ§¹ Limpeza dos Buckets ETL

Antes de destruir a infraestrutura:

```bash
# Esvaziar todos os buckets ETL
aws s3 rm s3://aej-staging-bucket-XXXXXX --recursive
aws s3 rm s3://aej-trusted-bucket-XXXXXX --recursive
aws s3 rm s3://aej-cured-bucket-XXXXXX --recursive

# Depois destruir
terraform destroy -auto-approve
```

**Ou configure `force_destroy = true`** (jÃ¡ configurado por padrÃ£o nos buckets ETL).

### ğŸ› Troubleshooting ETL

**Problema**: Lambda nÃ£o estÃ¡ processando
- Verifique se o arquivo Ã© `.csv` (Excel requer layer pandas)
- Confirme que enviou para o bucket correto (staging)
- Verifique logs no CloudWatch

**Problema**: Colunas nÃ£o encontradas
- O Lambda normaliza nomes (case-insensitive)
- Verifique se as colunas existem no CSV original
- Veja logs para mapeamento de colunas

**Problema**: Nenhum livro no Cured
- Verifique se `product_category_name` contÃ©m 'livro', 'book' ou 'literatura'
- Veja estatÃ­sticas nos logs do Lambda

**Problema**: Arquivo muito grande
- Aumente `timeout` e `memory_size` dos Lambdas no Terraform
- Considere dividir arquivos grandes em chunks menores

### ğŸ’¡ Dicas de Uso

1. **Use CSV em vez de Excel** para melhor performance e compatibilidade
2. **Monitore os logs** durante o primeiro processamento para validar mapeamento de colunas
3. **Teste com arquivo pequeno** primeiro para validar o pipeline
4. **Revise os outputs** do Terraform para ver nomes dos buckets criados

---
